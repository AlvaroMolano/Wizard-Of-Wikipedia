{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"retrieval_model_1.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"YH_-fmWzooRD","colab_type":"code","outputId":"daba5337-c3a2-49de-ce59-59f8e54a6378","executionInfo":{"status":"ok","timestamp":1563906865755,"user_tz":-330,"elapsed":2076,"user":{"displayName":"deepak goyal","photoUrl":"","userId":"05164064759516400423"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["\"\"\"\n","Retrieval Model for \"Wizard Of Wikipedia\" task based on Dinan et al. Wizard Of Wikipedia: Knowledge\n","Powered Conversational Agents.\n","\n","Deepak Goyal <deepak.16je002137@ece.iitism.ac.in>\n","\n","\"\"\""],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nRetrieval Model for \"Wizard Of Wikipedia\" task based on Dinan et al. Wizard Of Wikipedia: Knowledge\\nPowered Conversational Agents.\\n\\nDeepak Goyal <deepak.16je002137@ece.iitism.ac.in>\\n\\n'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"4wgPWL8BowJG","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"16uafNo9ooRJ","colab_type":"code","colab":{}},"source":["\"\"\"\n","This code takes inspiration from the following sources:\n","\n","The Annotated Transformer : http://nlp.seas.harvard.edu/2018/04/03/attention.html\n","Illustrated Transformer   : http://jalammar.github.io/illustrated-transformer/\n","Samuel Lynn Evans's Blog  : https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec\n","Tensorflow's Medium Blog  : https://medium.com/tensorflow/a-transformer-chatbot-tutorial-with-tensorflow-2-0-88bf59e66fe2\n","Pytorch's Chatbot Tutorial: https://pytorch.org/tutorials/beginner/chatbot_tutorial.html\n","\n","\"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K4hTusQoooRM","colab_type":"code","colab":{}},"source":["# Framework for running model on GPU\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# For Visualising gradient flow across the model\n","import matplotlib.pyplot as plt\n","from matplotlib.lines import Line2D\n","\n","# For Visualising training loss, accuracy Vs epochs\n","# from visdom import Visdom\n","\n","# Basic computation library needed for visdom visualising class\n","import numpy as np\n","\n","# Other libraries for data loading, processing etc\n","import json\n","import os\n","import re\n","import math\n","import copy\n","import os\n","import string\n","import time"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pDseLSvOooRP","colab_type":"code","colab":{}},"source":["# Specify which machine you are running things\n","running_device='colab'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KWGbXKU5ooRS","colab_type":"code","colab":{}},"source":["def raw_data_loader(running_device):\n","    \"\"\" \n","        Function for loading json data into a list\n","    \"\"\"\n","    \n","    file_dir=''\n","    file_name='data.json'\n","    if running_device=='local':\n","        file_dir='/home/naive/Documents/rohit/Wizard Of Wikipedia/Dataset'\n","    elif running_device=='colab':\n","        file_dir='/content/drive/My Drive/Data/Wizard of Wikipedia/wizard_of_wikipedia'\n","    else:\n","        print(\"Invalid running device\")\n","        return\n","    \n","    data=os.path.join(file_dir,file_name)\n","    \n","    json_data=None\n","    with open(data) as f:\n","        json_data=json.load(f)\n","    \n","    return json_data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s-EQwC3nooRU","colab_type":"code","colab":{}},"source":["json_data=None\n","\n","print(\"Loading Raw Data into a list....\")\n","\n","t1=time.time()\n","json_data=raw_data_loader(running_device)\n","t2=time.time()\n","\n","print(\"Loading raw data took \"+str(t2-t1)+\" seconds\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"olP73ydKooRY","colab_type":"code","colab":{}},"source":["class vocabulary:\n","    \"\"\" Creates vocabulary for our corpus data of conversations\"\"\"\n","    \n","    def __init__(self):\n","        \"\"\"\n","            self.word2Index(Dictionary): Maps words to tokens\n","            self.num_words(Scalar)     : Number of distinct words in the vocabulary\n","            \n","        \"\"\"\n","        \n","        self.word2Index={'PAD':0,\"<START>\":1,\"<END>\":2}\n","        self.num_words=3\n","        \n","    def addSentence(self,sentence):\n","        \"\"\" \n","            Adds sentences into vocabulary by splitting them\n","        \"\"\"\n","        \n","        for word in sentence.split(\" \"):\n","            self.addWord(word)\n","    \n","    def addWord(self,word):\n","        \n","        if word not in self.word2Index.keys():\n","            self.word2Index[word]=self.num_words\n","            self.num_words+=1       "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yQ0HkTGmooRa","colab_type":"code","colab":{}},"source":["# Preprocessing functions for the data\n","\n","def remove_punctuation(sentence):\n","    # thanks to https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string\n","    return sentence.translate(str.maketrans('', '', string.punctuation)).lower().strip()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pLCXnaL3ooRd","colab_type":"code","colab":{}},"source":["def load_data(data_json):\n","    \n","    context, knowledge, wizard=[],[],[]\n","    dict2cnt={}\n","    count=0\n","    \n","    N=len(data_json)\n","    for id in range(N):\n","        \n","        current_knowledge,current_wizard,context_knowledge,previous_context=[],[],[],[]\n","        tmp_context=[]\n","        \n","        current_conv=data_json[id]\n","        \n","        topic_chosen=remove_punctuation(current_conv['chosen_topic'])\n","        tmp_context.append(topic_chosen)\n","        \n","        conversation_length=len(current_conv['dialog'])\n","        \n","        for i in range(conversation_length):\n","            \n","            if current_conv['dialog'][i]['speaker']=='0_Wizard':\n","                \n","                dict2cnt[count]=len(current_conv['dialog'][i]['retrieved_passages'])\n","                count=count+1\n","                \n","                for x in range(len(current_conv['dialog'][i]['retrieved_passages'])):\n","                    \n","                    idx=current_conv['dialog'][i]['retrieved_passages'][x]\n","                    \n","                    for value in idx.values():\n","                        \n","                        for j in range(len(value)):\n","                            value[j]=remove_punctuation(value[j])\n","                            \n","                        current_knowledge.append(value)\n","                \n","                wizard_dialog=remove_punctuation(current_conv['dialog'][i]['text'])\n","                \n","                current_wizard.append(wizard_dialog)\n","                tmp_context.append(wizard_dialog)\n","            \n","            else:\n","                apperentice_dialog=remove_punctuation(current_conv['dialog'][i]['text'])\n","                tmp_context.append(apperentice_dialog)\n","                \n","        knowledge.append(current_knowledge)\n","        wizard.append(current_wizard)\n","        \n","        final_context=[]\n","        flag=True\n","        \n","        if current_conv['dialog'][0]['speaker']=='0_Wizard':\n","            flag=False\n","        \n","        for i in range(len(tmp_context)):\n","            temp=[]\n","            \n","            for j in range(i+1):\n","                temp.append(tmp_context[j])\n","                \n","            if flag:\n","                if i%2!=0 and len(final_context)<len(current_wizard):\n","                    final_context.append(temp)\n","                    \n","            else:\n","                if i%2==0 and len(final_context)<len(current_wizard):\n","                    final_context.append(temp)\n","        \n","        context.append(final_context)\n","    \n","    return context, knowledge, wizard, dict2cnt\n","\n","print(\"Getting 22311 conversation's context, knowledge, wizard utterances..\")\n","t1=time.time()\n","context, knowledge, wizard, dict2cnt=load_data(json_data)        \n","t2=time.time()\n","print(\"This took: \"+str(t2-t1)+\" seconds\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CBJ3xy_oooRh","colab_type":"code","colab":{}},"source":["def after_load_process(context,knowledge,wizard):\n","    input_context=[]\n","    input_knowledge=[]\n","    output_wizard=[]\n","    for i in range(len(wizard)):\n","        for c in context[i]:\n","            input_context.append(c)\n","        for k in knowledge[i]:\n","            input_knowledge.append(k)\n","        for w in wizard[i]:\n","            output_wizard.append(w)\n","\n","    input_knowledge2=[]\n","    index=0\n","    for i in range(50246):\n","        r=dict2cnt[i]\n","        tmp=[]\n","        fr=index\n","        to=index+r\n","        for j in range(fr,to):\n","            tmp.append(input_knowledge[index])\n","            index=index+1\n","        input_knowledge2.append(tmp)\n","    input_knowledge3=[]\n","    for i in range(len(input_knowledge2)):\n","        g=[]\n","        for j in range(len(input_knowledge2[i])):\n","            s=\"\"\n","            for k in range(len(input_knowledge2[i][j])):\n","                s=s+\" \"+input_knowledge2[i][j][k]\n","            s=s.strip()\n","            g.append(s)\n","        input_knowledge3.append(g)\n","    return input_context,input_knowledge3,output_wizard\n","\n","print(\"Converting the extracted sentences into lists...\")\n","t1=time.time()\n","final_context, final_knowledge, final_wizard=after_load_process(context,knowledge,wizard)\n","t2=time.time()\n","print(\"This took: \"+str(t2-t1)+\" seconds\")\n","    \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iTe_YV13ooRk","colab_type":"code","colab":{}},"source":["##############################Sentence Length Analysis#############################\n","\n","avg_wizard=0.0\n","no_wizard=0\n","no_knowledge=0\n","no_context=0\n","for i in range(len(final_wizard)):\n","    avg_wizard+=len(final_wizard[i].split())\n","    no_wizard+=1\n","    \n","avg_context=0.0\n","for i in range(len(final_context)):\n","    for j in range(len(final_context[i])):\n","        avg_context+=len(final_context[i][j].split())\n","        no_context+=1\n","        \n","avg_knowledge=0.0\n","\n","for i in range(len(final_knowledge)):\n","    for j in range(len(final_knowledge[i])):\n","        avg_knowledge+=len(final_knowledge[i][j].split())\n","        no_knowledge+=1\n","        \n","avg_sentence=(avg_knowledge+avg_wizard+avg_context)/(no_wizard+no_context+no_knowledge)\n","print(\"Average Sentence Length: \"+str(avg_sentence))\n","avg_wizard=avg_wizard/len(final_wizard)\n","avg_context=avg_context/no_context\n","avg_knowledge=avg_knowledge/no_knowledge\n","print(\"Average wizard's sentence length: \"+str(avg_wizard))\n","print(\"Average knowledge's sentence length: \"+str(avg_knowledge))\n","print(\"Average context's sentence length: \"+str(avg_context))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sJ87vQuwooRo","colab_type":"code","colab":{}},"source":["max_length=70\n","\n","def process(sentence):\n","    \"\"\"\n","        Preprocessing sentencing to make them of equal length and appending and terminating them with\n","        start (<START>) and end (<END>) token.\n","        \n","    \"\"\"\n","    \n","    \n","    words=sentence.split()\n","    sentence_length=max_length-2\n","    if len(words)<=sentence_length:\n","        for i in range(sentence_length-len(words)):\n","            words.append(\"PAD\")\n","    else:\n","        words=words[:sentence_length]\n","    \n","    res=\"\"\n","    words.append(\"<END>\")\n","    words.insert(0,\"<START>\")\n","    assert len(words)==max_length\n","    for w in words:\n","        res=res+\" \"+w\n","    res=res.strip()\n","    return res"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ED0VllAYooRq","colab_type":"code","colab":{}},"source":["vocab=vocabulary()\n","\n","def process_loaded(input_context,input_knowledge,output_wizard):\n","    \"\"\" Processing sentences and forming vocabulary\n","    \"\"\"\n","    \n","    for i in range(len(output_wizard)):\n","        output_wizard[i]=process(output_wizard[i])\n","        vocab.addSentence(output_wizard[i])\n","        assert len(output_wizard[i].split())==max_length\n","    \n","    for i in range(len(input_knowledge)):\n","        for j in range(len(input_knowledge[i])):\n","            input_knowledge[i][j]=process(input_knowledge[i][j])\n","            vocab.addSentence(input_knowledge[i][j])\n","            assert len(input_knowledge[i][j].split())==max_length\n","        \n","    for i in range(len(input_context)):\n","        for j in range(len(input_context[i])):\n","            input_context[i][j]=process(input_context[i][j])\n","            vocab.addSentence(input_context[i][j])\n","            assert len(input_context[i][j].split())==max_length\n","    return input_context, input_knowledge, output_wizard\n","\n","print(\"Processing fetched sentences...\")\n","t1=time.time()\n","p_context,p_knowledge,p_wizard=process_loaded(final_context,final_knowledge,final_wizard)\n","t2=time.time()\n","print(\"This process took: \"+str(t2-t1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pCZsgihzooRu","colab_type":"code","colab":{}},"source":["##########################SANITY CHECK##########################\n","\n","assert len(p_wizard)==50246\n","assert len(p_context)==50246\n","assert len(p_knowledge)==50246\n","\n","print(\"Running Sanity Checks on the data...\")\n","\n","t1=time.time()\n","\n","for i in range(len(p_wizard)):\n","    assert len(p_wizard[i].split())==max_length\n","    \n","for i in range(len(p_context)):\n","    for j in range(len(p_context[i])):\n","        assert len(p_context[i][j].split())==max_length\n","        \n","for i in range(len(p_knowledge)):\n","    for j in range(len(p_knowledge[i])):\n","        assert len(p_knowledge[i][j].split())==max_length\n","\n","t2=time.time()\n","print(\"Everything seems fine.\")        \n","print(\"Sanity Checks took: \"+str(t2-t1)+\" seconds\")\n"," \n","################################################################"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GzIyY0sgooRy","colab_type":"code","colab":{}},"source":["# # For Visualising training loss per epochs etc\n","\n","# from visdom import Visdom\n","\n","# class VisdomLinePlotter(object):\n","#     \"\"\"Plots to Visdom\"\"\"\n","#     def __init__(self, env_name='main'):\n","#         self.viz = Visdom()\n","#         self.env = env_name\n","#         self.plots = {}\n","#     def plot(self, var_name, split_name, title_name, x, y):\n","#         if var_name not in self.plots:\n","#             self.plots[var_name] = self.viz.line(X=np.array([x,x]), Y=np.array([y,y]), env=self.env, opts=dict(\n","#                 legend=[split_name],\n","#                 title=title_name,\n","#                 xlabel='Epochs',\n","#                 ylabel=var_name\n","#             ))\n","#         else:\n","#             self.viz.line(X=np.array([x]), Y=np.array([y]), env=self.env, win=self.plots[var_name], name=split_name, update = 'append')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lp78nHTpooR2","colab_type":"code","colab":{}},"source":["device=torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NX5ZPNacooR7","colab_type":"code","colab":{}},"source":["class Embeddings(nn.Module):\n","    \"\"\" Embeddings class for creating pretrained embeddings for word's token\n","    \"\"\"\n","    def __init__(self, d_model, vocab_size,max_length):\n","        \"\"\" Class for converting sentence into matrix of word embeddings plus the positional encoding.\n","            \n","            Inputs: \n","                d_model(scalar)   : The size of encoding desired in the model.\n","                vocab_size(scalar): Size of the vocabulary of the model.\n","                max_length(scalar): Length of the sentences.\n","                \n","            Layers:\n","                self.embed: Embedding layer to convert tokens into embeddings.\n","                self.pe   : Positional Encoder encoding word's position in the sentence.\n","        \"\"\"\n","        super().__init__()\n","        self.embed = nn.Embedding(vocab_size, d_model)\n","        self.pe=PositionalEncoder(d_model,max_length)\n","        \n","    def forward(self, x):\n","        \"\"\" self.embed takes in the tokens associated with the words in the sentence for encoding into\n","            the embeddings.\n","            Input:\n","                x: Tensor representing sentences. x[i,:] is the vector of tokens of words of ith sentence.\n","                   size: (number_sentences,sentence_length)\n","            Output:\n","                y: Tensor representing each word's embedding in the sentence along with the added with positional encoding\n","                   size: (number_sentences,sentence_length,d_model)\n","        \"\"\"\n","\n","        y=self.embed(x)\n","        y=self.pe(y)\n","        \n","        return y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d2wgyyEiooR9","colab_type":"code","colab":{}},"source":["class PositionalEncoder(nn.Module):\n","    \"\"\" To encode the position of a particular word in the sentence, Positional Encoder adds \n","        something to each embedding position of a word wrt to its position in the sentence.\n","    \"\"\"\n","    def __init__(self,d_model,max_length=200):\n","        \"\"\" \n","            Input:\n","                d_model(scalar)        : The word embedding size of the model.\n","                max_length(scalar)     : The sentence length.\n","                \n","            Layers:\n","                self.pe: A tensor containing the values to be added to each word embedding that depends on \n","                         the position of the word in the sentence.\n","                         Good Explanation: https://datascience.stackexchange.com/questions/51065/what-is-positional-encoding-in-transformer-model?rq=1\n","        \"\"\"\n","        super().__init__()\n","        self.d_model=d_model\n","        self.pe=torch.zeros(max_length,d_model,requires_grad=False,device=device)\n","        for pos in range(max_length):\n","            for i in range(0,d_model,2):\n","                self.pe[pos,i]=\\\n","                math.sin(pos/(10000**((2*i)/d_model)))\n","                self.pe[pos,i+1]=\\\n","                math.cos(pos/(10000**((2*(i+1))/d_model)))\n","        \n","    def forward(self,x):\n","        \"\"\"\n","            Input:\n","                x: Matrix representation of sentence.\n","                   size: sentence_length*d_model\n","            Output:\n","                x: Positional Encoded sentence.\n","                   size: sentence_length*d_model\n","        \"\"\"\n","        \n","        x=x*math.sqrt(self.d_model) # makes embeddings larger by multiplying with d_model\n","        x=x+self.pe\n","        \n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wgk1xQbVooSA","colab_type":"code","colab":{}},"source":["def attention(q, k, v, d_k, mask=None, dropout=None):\n","    \n","    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n","    if mask is not None:\n","        mask = mask.unsqueeze(1)\n","        scores = scores.masked_fill(mask == 0, -1e9)\n","    scores = F.softmax(scores, dim=-1)\n","      \n","    if dropout is not None:\n","        scores = dropout(scores)\n","        \n","    output = torch.matmul(scores, v)\n","    return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FmxEblS1ooSE","colab_type":"code","colab":{}},"source":["class MultiHeadAttention(nn.Module):\n","    \"\"\" Calculates Multihead attention as described in the \"Attention is all you need\"\n","        For more details checkout: http://jalammar.github.io/illustrated-transformer/\n","    \"\"\"\n","    def __init__(self, heads, d_model, dropout = 0.1):\n","        \"\"\" \n","            Inputs:\n","                heads(scalar)  : Number of heads to split the d_model.\n","                d_model(scalar): Size of the word embeddings used in the model\n","                dropout(scalar): Probability for dropout layers\n","            \n","            Layers:\n","                self.q_linear: Linear layer for queries\n","                self.v_linear: Linear layer for values\n","                self.k_linear: Linear layer for keys\n","                self.dropout : Dropout layer  \n","                \n","        \"\"\"\n","        super().__init__()\n","        \n","        assert d_model%heads==0\n","        \n","        self.d_model = d_model\n","        self.d_k = d_model// heads\n","        self.h = heads\n","        \n","        self.q_linear = nn.Linear(d_model, d_model)\n","        self.v_linear = nn.Linear(d_model, d_model)\n","        self.k_linear = nn.Linear(d_model, d_model)        \n","        self.dropout = nn.Dropout(dropout)\n","        self.out = nn.Linear(d_model, d_model)\n","    \n","    def forward(self, q, k, v,mask=None):\n","        \"\"\"\n","            Inputs:\n","                q   : Query matrix for self attention\n","                      size: (sentence_length,d_model)\n","                k   : Key matrix for self attention\n","                      size: (sentence_length,d_model)\n","                v   : Value matrix for self attention\n","                      size: (sentence_length,d_model)\n","                mask: to make model blind for ahead words used in decoder\n","                      size: (sentence_length,sentence_length)\n","                      \n","             Outputs:\n","                 output: The final output of the multi head attention as described in \n","                         \"Attention is all you need\"\n","                         size: (sentence_length,d_model)\n","        \"\"\"\n","        \n","        bs = q.size(0)      \n","        \n","        k = self.k_linear(k).view(bs,-1,self.h,self.d_k)\n","        q = self.q_linear(q).view(bs,-1,self.h,self.d_k)\n","        v = self.v_linear(v).view(bs,-1,self.h,self.d_k)\n","        \n","        k=k.transpose(1,2)\n","        q=q.transpose(1,2)\n","        v=v.transpose(1,2)\n","                \n","        scores = attention(q, k, v, self.d_k,mask,self.dropout)\n","        \n","        concat = scores.transpose(1,2).contiguous()\\\n","        .view(bs,-1,self.d_model)\n","        \n","        output = self.out(concat)\n","\n","        return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ttdl2C-kooSJ","colab_type":"code","colab":{}},"source":["class FeedForward(nn.Module):\n","    \"\"\" FeedForward class consist of two linear layer.\n","    \"\"\"\n","    def __init__(self,d_model,d_ff=2048,dropout=0.1):\n","        \"\"\"\n","            Input:\n","                d_model(scalar): Model's embedding size\n","                d_ff(scalar)   : First Linear layer output neurons\n","                dropout(scalar): Dropout probability for dropout layer\n","            \n","            Layers:\n","                self.linear1: First linear layer\n","                self.dropout: Dropout layer (A simple way to prevent overfitting by Srivastava et al)\n","                              http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\n","                self.linear2: Second linear layer\n","        \"\"\"\n","        super().__init__()\n","        \n","        self.linear1=nn.Linear(d_model,d_ff)\n","        self.dropout=nn.Dropout(dropout)\n","        self.linear2=nn.Linear(d_ff,d_model)\n","        \n","    def forward(self,x):\n","        \"\"\"\n","            Input:\n","                x: Output of the multihead attention plus the residual\n","                   size: (sentence_length,d_model)\n","                   \n","            Output:\n","                x: Output of the two linear layer with relu in between as the non linearity\n","                   size: (sentence_length,d_model)\n","        \"\"\"\n","        x=self.dropout(F.relu(self.linear1(x)))\n","        x=self.linear2(x)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GdFa13j7ooSM","colab_type":"code","colab":{}},"source":["class Encoder(nn.Module):\n","    \"\"\"Encoder Module of the Transformer Model\"\"\"\n","    def __init__(self,d_model,d_ff,dropout,h):\n","        \"\"\"\n","            Inputs: \n","                d_model(scalar): The size of the word embedding for the model.\n","                d_ff(scalar)   : The size of the output neurons of the first linear in feed forward layer.\n","                dropout(scalar): The probability for the dropout layer\n","                h(scalar)      : Number of heads to split the incoming data. Make sure d_model%h==0\n","            \n","            Layers:\n","                self.mha       : The multihead attention sublayer of the encoder\n","                self.ff        : Feed Forward sublayer of the encoder \n","                self.dropout1  : Dropout layer for the output of multiheadattention layer\n","                self.dropout2  : Dropout layer for the output of feedforward layer\n","        \"\"\"\n","        \n","        \n","        super().__init__()\n","        \n","        self.mha=MultiHeadAttention(h,d_model,dropout)\n","        self.ff=FeedForward(d_model,d_ff,dropout)\n","        self.dropout1=nn.Dropout(dropout)\n","        self.dropout2=nn.Dropout(dropout)\n","        \n","    def forward(self,y,mask):\n","        \"\"\"\n","            Inputs:\n","                y   : The matrix representing the input sentence into the encoder\n","                      size: (max_length,d_model)\n","                mask: The matrix for masking the model to not see ahead in decoder\n","                      size: None in case of encoder\n","                      \n","            Output:\n","                y   : The output of the multihead attention and residual layer --> Feedforward plus \n","                      the residual.\n","                      size: (max_length,d_model)\n","        \"\"\"\n","        y=self.dropout1(self.mha(y,y,y,mask))+y\n","        return self.dropout2(self.ff(y))+y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UNUWbOAsooSP","colab_type":"code","colab":{}},"source":["\n","class RetrievalModel(nn.Module):\n","\n","    def __init__(\n","        self,\n","        d_model,\n","        d_ff,\n","        dropout,\n","        h,\n","        vocab_size,\n","        max_length,\n","        ):\n","        \n","        super().__init__()\n","\n","        self.embed = Embeddings(d_model, vocab_size, max_length)\n","        self.embed.embed.weight.requires_grad = False\n","\n","        self.encoderLayer = Encoder(d_model, d_ff, dropout, h)\n","        self.d_model = d_model\n","        self.length = max_length\n","\n","    def forward(\n","        self,\n","        x,\n","        k,\n","        r,\n","        ):\n","\n","        X = self.encoderLayer(self.embed(x), None).view(x.size()[0], -1)\n","        K = self.encoderLayer(self.embed(k), None).view(k.size()[0], -1)\n","        R = self.encoderLayer(self.embed(r), None).view(r.size()[0], -1)\n","\n","\n","\n","        Y = X.matmul(K.t())\n","        func = nn.Softmax(dim=-1)\n","        Y = func(Y)\n","\n","        attnd = X + Y.matmul(K)\n","        Z=attnd.mean(dim=0)\n","\n","        const = torch.norm(Z)       \n","        Z = torch.div(Z, const)\n","        \n","        normv = torch.norm(R)\n","        res = torch.matmul(R, Z.t())\n","        res = torch.div(res, normv)\n","        \n","        return res\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9qkttD6CooSS","colab_type":"code","colab":{}},"source":["def plot_grad_flow(named_parameters):\n","    \"\"\"\n","        Plotting gradient flow across various layers\n","        Thanks to: https://discuss.pytorch.org/t/check-gradient-flow-in-network/15063/2\n","    \"\"\"   \n","    ave_grads = []\n","    layers = []\n","    for n, p in named_parameters:\n","        if(p.requires_grad) and (\"bias\" not in n):\n","            layers.append(n)\n","            ave_grads.append(p.grad.abs().mean())\n","    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n","    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n","    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n","    plt.xlim(xmin=0, xmax=len(ave_grads))\n","    plt.xlabel(\"Layers\")\n","    plt.ylabel(\"average gradient\")\n","    plt.title(\"Gradient flow\")\n","    plt.grid(True)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H_qSMeQuooSU","colab_type":"code","colab":{}},"source":["def embed(s):\n","    \n","    \"\"\" Converting sentences into a list of tokens\n","    \"\"\"\n","    i=0\n","    wordList=s.split()\n","    \n","    assert len(wordList)==max_length\n","    \n","    res=torch.zeros(len(wordList))\n","    \n","    for w in wordList:\n","        idx=vocab.word2Index[w]\n","        res[i]=idx\n","        i=i+1\n","    return res\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TBGCJZZ2ooSY","colab_type":"code","colab":{}},"source":["def run_epoch(context,knowledge,wizard,model,nepochs):\n","    \n","  start=time.time()\n","\n","  total_loss=0\n","  avg_acc=0\n","    \n","  data_len=5 # batch size\n","  nbatches=10 # Number of batches\n","  epochs=nepochs # Number of epochs  \n","       \n","  optimizer=torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),lr=0.1,betas=(0.9,0.98),eps=1e-9,amsgrad=True)\n","#   model_opt = NoamOpt(512, 1, 400,\n","#         torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9,amsgrad=True))\n","\n","  \n","  \n","  for i in range(epochs):  #epoch\n","    \n","    model.train()\n","    acc=0\n","    loss=0\n","    \n","    for j in range(nbatches): #batch\n","      wizard_em=torch.zeros(data_len,max_length,dtype=torch.long)\n","    \n","      counter_epoch=j*data_len\n","      for k in range(data_len):\n","        wizard_em[k,:]=(embed(wizard[counter_epoch+k]))\n","        \n","      target=torch.zeros(data_len,dtype=torch.long)\n","      inp=torch.zeros(data_len,data_len)\n","      \n","      for k in range(data_len):\n","        \n","        context_em=torch.zeros(len(context[counter_epoch+k]),max_length,dtype=torch.long)\n","        \n","        for l in range(len(context[counter_epoch+k])):\n","          context_em[l,:]=(embed(context[counter_epoch+k][l]))\n","\n","        knowledge_em=torch.zeros(len(knowledge[counter_epoch+k]),max_length,dtype=torch.long)\n","        \n","        for l in range(len(knowledge[counter_epoch])):\n","          knowledge_em[l,:]=(embed(knowledge[counter_epoch+k][l]))\n","        \n","        out=model(context_em,knowledge_em,wizard_em)\n","        inp[k,:]=out.t()\n","        target[k]=k\n","        \n","        if torch.argmax(out).item()==k:\n","          acc=acc+1\n","\n","      loss_curr=nn.CrossEntropyLoss()\n","      output_loss=loss_curr(inp,target)\n","        \n","      optimizer.zero_grad()\n","#       model_opt.optimizer.zero_grad()\n","      output_loss.backward()\n","        \n","      plot_grad_flow(model.named_parameters()) \n","#       nn.utils.clip_grad_norm_(model.parameters(),1)\n","      optimizer.step()\n","#       model_opt.step()\n","     \n","      loss=loss+output_loss.item()\n","      torch.cuda.empty_cache()\n","      \n","    loss=loss/nbatches\n","#     plotter.plot(\"epoch Loss\",\"epoch-number\",\"Loss Vs Epochs ADAM \",i,loss)\n","   \n","\n","    \n","    print(\"-\"*80)\n","    print(\"Epoch: \"+str(i)+\" \"+str((acc*100)/(nbatches*data_len)))\n","    avg_acc=avg_acc+(acc*100)/(nbatches*data_len)\n","    print(loss)\n","    print(\"-\"*80)\n","\n","    elapsed=time.time()-start\n","    start=time.time()\n","    print(elapsed)\n","    print(\"-\"*80)\n","    \n","  print(\"Average Accuracy: \"+str(avg_acc/epochs))\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SKjh86_booSb","colab_type":"code","colab":{}},"source":["# plotter=VisdomLinePlotter('main')\n","model=RetrievalModel(256,1024,0.1,8,vocab.num_words,max_length)\n","model.cuda()\n","torch.set_default_tensor_type('torch.cuda.FloatTensor')\n","for p in model.parameters():\n","    if p.dim()>1:\n","        nn.init.xavier_uniform_(p)\n","run_epoch(p_context,p_knowledge,p_wizard,model,1000)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5SNb_d6FooSe","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}